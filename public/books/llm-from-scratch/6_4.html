<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PyTorch Dataset & DataLoader Pipeline</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background: linear-gradient(135deg, #fafbff 0%, #f5f7ff 100%);
            min-height: 100vh;
            padding: 15px;
            color: #1e293b;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
        }

        header {
            text-align: center;
            margin-bottom: 15px;
        }

        .steps-overview {
            background: white;
            border-radius: 16px;
            padding: 25px;
            margin-bottom: 20px;
            box-shadow: 0 10px 30px rgba(139, 92, 246, 0.1);
        }

        .overview-title {
            font-size: 1.1rem;
            font-weight: 600;
            color: #1e293b;
            margin-bottom: 15px;
            text-align: center;
        }

        .steps-grid {
            display: grid;
            grid-template-columns: repeat(7, 1fr);
            gap: 12px;
        }

        .step-card {
            background: linear-gradient(135deg, #f8fafc 0%, #f1f5f9 100%);
            border: 2px solid #e2e8f0;
            border-radius: 12px;
            padding: 15px 10px;
            text-align: center;
            cursor: pointer;
            transition: all 0.3s ease;
            position: relative;
        }

        .step-card:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 20px rgba(139, 92, 246, 0.2);
            border-color: #8b5cf6;
        }

        .step-card.active-card {
            background: linear-gradient(135deg, #8b5cf6 0%, #a78bfa 100%);
            border-color: #7c3aed;
            color: white;
        }

        .step-card.active-card .step-card-emoji {
            transform: scale(1.2);
        }

        .step-card.active-card .step-card-title {
            color: white;
        }

        .step-card.active-card .step-card-number {
            background: rgba(255, 255, 255, 0.3);
            color: white;
        }

        .step-card-number {
            background: #8b5cf6;
            color: white;
            width: 24px;
            height: 24px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.75rem;
            font-weight: 700;
            margin: 0 auto 8px auto;
        }

        .step-card-emoji {
            font-size: 1.8rem;
            display: block;
            margin-bottom: 8px;
            transition: transform 0.3s ease;
        }

        .step-card-title {
            font-size: 0.75rem;
            font-weight: 600;
            color: #1e293b;
            line-height: 1.3;
        }

        @media (max-width: 1200px) {
            .steps-grid {
                grid-template-columns: repeat(4, 1fr);
            }
        }

        @media (max-width: 768px) {
            .steps-grid {
                grid-template-columns: repeat(2, 1fr);
            }
        }

        h1 {
            font-size: 1.8rem;
            color: #1e293b;
            margin-bottom: 8px;
        }

        .subtitle {
            color: #64748b;
            font-size: 0.95rem;
        }

        .main-content {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-bottom: 20px;
        }

        .visualization-area {
            background: white;
            border-radius: 16px;
            padding: 25px;
            box-shadow: 0 10px 30px rgba(139, 92, 246, 0.1);
            min-height: 400px;
            display: flex;
            align-items: center;
            justify-content: center;
            position: relative;
        }

        .explanation-area {
            background: white;
            border-radius: 16px;
            padding: 25px;
            box-shadow: 0 10px 30px rgba(139, 92, 246, 0.1);
            overflow-y: auto;
            max-height: 550px;
        }

        .step-title {
            font-size: 1.3rem;
            color: #1e293b;
            margin-bottom: 12px;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .step-emoji {
            font-size: 1.4rem;
        }

        .explanation-text {
            color: #64748b;
            line-height: 1.6;
            font-size: 0.9rem;
        }

        .explanation-text h3 {
            color: #1e293b;
            margin: 12px 0 8px 0;
            font-size: 1rem;
        }

        .explanation-text ul {
            margin-left: 20px;
            margin-top: 6px;
        }

        .explanation-text li {
            margin-bottom: 5px;
        }

        .code-example {
            background: #0f172a;
            color: #e2e8f0;
            padding: 10px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            font-size: 0.8rem;
            margin: 10px 0;
            overflow-x: auto;
        }

        .view-code-btn {
            background: #8b5cf6;
            color: white;
            border: none;
            padding: 8px 16px;
            border-radius: 8px;
            font-size: 0.9rem;
            cursor: pointer;
            margin-top: 12px;
            transition: background 0.3s;
        }

        .view-code-btn:hover {
            background: #7c3aed;
        }

        .controls {
            background: white;
            border-radius: 16px;
            padding: 20px;
            box-shadow: 0 10px 30px rgba(139, 92, 246, 0.1);
        }

        .control-section {
            margin-bottom: 15px;
        }

        .control-label {
            font-weight: 600;
            color: #1e293b;
            margin-bottom: 8px;
            display: block;
            font-size: 0.9rem;
        }

        .step-navigation {
            display: flex;
            align-items: center;
            gap: 10px;
            flex-wrap: wrap;
        }

        .nav-btn, .step-btn {
            background: #e2e8f0;
            border: none;
            padding: 8px 12px;
            border-radius: 8px;
            cursor: pointer;
            transition: all 0.3s;
            font-size: 0.9rem;
        }

        .nav-btn:hover {
            background: #cbd5e1;
        }

        .step-btn {
            width: 35px;
            height: 35px;
            padding: 0;
        }

        .step-btn.active {
            background: #8b5cf6;
            color: white;
        }

        .play-pause-btn {
            background: #14b8a6;
            color: white;
            padding: 8px 16px;
        }

        .play-pause-btn:hover {
            background: #0d9488;
        }

        .slider-container {
            display: flex;
            align-items: center;
            gap: 15px;
        }

        .slider {
            flex: 1;
            height: 8px;
            border-radius: 4px;
            background: #e2e8f0;
            outline: none;
            -webkit-appearance: none;
        }

        .slider::-webkit-slider-thumb {
            -webkit-appearance: none;
            appearance: none;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            background: #8b5cf6;
            cursor: pointer;
        }

        .slider::-moz-range-thumb {
            width: 20px;
            height: 20px;
            border-radius: 50%;
            background: #8b5cf6;
            cursor: pointer;
            border: none;
        }

        .slider-value {
            font-weight: 600;
            color: #8b5cf6;
            min-width: 30px;
        }

        .toggle-switch {
            position: relative;
            display: inline-block;
            width: 60px;
            height: 30px;
        }

        .toggle-switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }

        .toggle-slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #cbd5e1;
            transition: 0.4s;
            border-radius: 30px;
        }

        .toggle-slider:before {
            position: absolute;
            content: "";
            height: 22px;
            width: 22px;
            left: 4px;
            bottom: 4px;
            background-color: white;
            transition: 0.4s;
            border-radius: 50%;
        }

        input:checked + .toggle-slider {
            background-color: #8b5cf6;
        }

        input:checked + .toggle-slider:before {
            transform: translateX(30px);
        }

        /* Modal Styles */
        .modal {
            display: none;
            position: fixed;
            z-index: 1000;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0, 0, 0, 0.5);
            animation: fadeIn 0.3s;
        }

        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .modal-content {
            background-color: white;
            margin: 5% auto;
            padding: 0;
            border-radius: 16px;
            width: 80%;
            max-width: 900px;
            max-height: 80vh;
            display: flex;
            flex-direction: column;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
        }

        .modal-header {
            padding: 20px 30px;
            border-bottom: 1px solid #e2e8f0;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .modal-header h2 {
            color: #1e293b;
        }

        .close {
            color: #64748b;
            font-size: 28px;
            font-weight: bold;
            cursor: pointer;
            transition: color 0.3s;
        }

        .close:hover {
            color: #1e293b;
        }

        .modal-body {
            padding: 30px;
            overflow-y: auto;
            flex: 1;
        }

        .code-container {
            background: #0f172a;
            border-radius: 8px;
            overflow: hidden;
        }

        .code-line {
            display: flex;
            padding: 4px 0;
            transition: all 0.3s;
        }

        .line-number {
            color: #64748b;
            padding: 0 15px;
            text-align: right;
            min-width: 50px;
            user-select: none;
        }

        .line-content {
            color: #e2e8f0;
            padding-right: 15px;
            flex: 1;
            white-space: pre;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
        }

        .code-line.highlight {
            border-left: 4px solid #f97316;
        }

        .code-line.highlight .line-content {
            background-color: rgba(249, 115, 22, 0.08);
        }

        /* Visualization Elements */
        .viz-container {
            width: 100%;
            height: 100%;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            gap: 20px;
        }

        .sms-box {
            background: linear-gradient(135deg, #f97316 0%, #fb923c 100%);
            color: white;
            padding: 12px 16px;
            border-radius: 12px;
            box-shadow: 0 4px 15px rgba(249, 115, 22, 0.3);
            max-width: 400px;
            margin: 8px;
            animation: slideIn 0.5s ease-out;
            font-size: 0.9rem;
        }

        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .token-flow {
            display: flex;
            align-items: center;
            gap: 15px;
            flex-wrap: wrap;
            justify-content: center;
        }

        .token {
            background: #8b5cf6;
            color: white;
            padding: 6px 10px;
            border-radius: 6px;
            font-family: monospace;
            font-size: 0.8rem;
            animation: fadeIn 0.5s ease-out;
        }

        .arrow {
            font-size: 1.5rem;
            color: #8b5cf6;
            animation: pulse 2s infinite;
        }

        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }

        .sequence-bar {
            display: flex;
            align-items: center;
            margin: 6px 0;
            animation: slideIn 0.5s ease-out;
        }

        .sequence-label {
            min-width: 80px;
            color: #64748b;
            font-weight: 600;
            font-size: 0.85rem;
        }

        .bar-container {
            flex: 1;
            height: 24px;
            background: #e2e8f0;
            border-radius: 6px;
            overflow: hidden;
            position: relative;
        }

        .bar-filled {
            height: 100%;
            background: linear-gradient(90deg, #8b5cf6 0%, #a78bfa 100%);
            transition: width 0.5s ease;
        }

        .bar-padding {
            height: 100%;
            background: #cbd5e1;
            position: absolute;
            right: 0;
            top: 0;
        }

        .length-label {
            margin-left: 8px;
            color: #64748b;
            font-family: monospace;
            font-size: 0.85rem;
        }

        .padding-demo {
            display: flex;
            flex-direction: column;
            gap: 15px;
            width: 100%;
            max-width: 600px;
        }

        .padding-sequence {
            background: white;
            border: 2px solid #e2e8f0;
            border-radius: 12px;
            padding: 15px;
        }

        .sequence-tokens {
            display: flex;
            gap: 4px;
            flex-wrap: wrap;
            margin-top: 8px;
        }

        .token-small {
            width: 35px;
            height: 25px;
            border-radius: 4px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.65rem;
            font-family: monospace;
        }

        .token-actual {
            background: #8b5cf6;
            color: white;
        }

        .token-pad {
            background: #e2e8f0;
            color: #94a3b8;
        }

        .dataset-diagram {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 20px;
        }

        .dataset-box {
            background: linear-gradient(135deg, #14b8a6 0%, #2dd4bf 100%);
            color: white;
            padding: 20px 30px;
            border-radius: 12px;
            box-shadow: 0 4px 15px rgba(20, 184, 166, 0.3);
            text-align: center;
            font-weight: 600;
            font-size: 1rem;
        }

        .batch-stack {
            display: flex;
            flex-direction: column;
            gap: 4px;
            align-items: center;
        }

        .batch-item {
            background: #c4b5fd;
            padding: 8px 24px;
            border-radius: 6px;
            color: #1e293b;
            font-family: monospace;
            font-size: 0.8rem;
            animation: stackIn 0.5s ease-out backwards;
        }

        .batch-item:nth-child(1) { animation-delay: 0.1s; }
        .batch-item:nth-child(2) { animation-delay: 0.2s; }
        .batch-item:nth-child(3) { animation-delay: 0.3s; }
        .batch-item:nth-child(4) { animation-delay: 0.4s; }

        @keyframes stackIn {
            from {
                opacity: 0;
                transform: translateY(-20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .shape-display {
            background: #fef3c7;
            border: 2px solid #fbbf24;
            border-radius: 8px;
            padding: 12px 20px;
            font-family: monospace;
            font-size: 0.95rem;
            color: #1e293b;
            font-weight: 600;
        }

        .dataloader-loop {
            display: flex;
            flex-direction: column;
            gap: 15px;
            align-items: center;
            width: 100%;
        }

        .loop-iteration {
            background: white;
            border: 2px solid #8b5cf6;
            border-radius: 12px;
            padding: 15px;
            width: 100%;
            max-width: 500px;
        }

        .iteration-header {
            color: #8b5cf6;
            font-weight: 600;
            margin-bottom: 8px;
            font-size: 0.9rem;
        }

        .fade-enter {
            animation: fadeIn 0.6s ease-out;
        }

        .fade-exit {
            animation: fadeOut 0.3s ease-out;
        }

        @keyframes fadeOut {
            from { opacity: 1; }
            to { opacity: 0; }
        }

        .key-takeaways {
            background: linear-gradient(135deg, #ecfdf5 0%, #d1fae5 100%);
            border-radius: 16px;
            padding: 20px;
            margin-top: 20px;
            border: 2px solid #10b981;
        }

        .key-takeaways h3 {
            color: #065f46;
            font-size: 1.1rem;
            margin-bottom: 12px;
        }

        .takeaway-list {
            list-style: none;
            padding: 0;
        }

        .takeaway-list li {
            padding: 6px 0;
            color: #064e3b;
            font-size: 0.9rem;
            display: flex;
            align-items: flex-start;
            gap: 8px;
        }

        .takeaway-list li:before {
            content: "‚úì";
            color: #10b981;
            font-weight: bold;
            font-size: 1rem;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>PyTorch Dataset & DataLoader Pipeline</h1>
            <p class="subtitle">Interactive visualization of text data preprocessing for deep learning</p>
        </header>

        <div class="steps-overview">
            <div class="overview-title">üìö Complete Pipeline Overview - Click any step to jump</div>
            <div class="steps-grid">
                <div class="step-card" onclick="goToStep(1)" data-step-card="1">
                    <div class="step-card-number">1</div>
                    <span class="step-card-emoji">üì•</span>
                    <div class="step-card-title">Raw SMS Text</div>
                </div>
                <div class="step-card" onclick="goToStep(2)" data-step-card="2">
                    <div class="step-card-number">2</div>
                    <span class="step-card-emoji">üî¢</span>
                    <div class="step-card-title">Tokenization</div>
                </div>
                <div class="step-card" onclick="goToStep(3)" data-step-card="3">
                    <div class="step-card-number">3</div>
                    <span class="step-card-emoji">üìè</span>
                    <div class="step-card-title">Max Length</div>
                </div>
                <div class="step-card" onclick="goToStep(4)" data-step-card="4">
                    <div class="step-card-number">4</div>
                    <span class="step-card-emoji">‚öñÔ∏è</span>
                    <div class="step-card-title">Padding</div>
                </div>
                <div class="step-card" onclick="goToStep(5)" data-step-card="5">
                    <div class="step-card-number">5</div>
                    <span class="step-card-emoji">üéØ</span>
                    <div class="step-card-title">__getitem__</div>
                </div>
                <div class="step-card" onclick="goToStep(6)" data-step-card="6">
                    <div class="step-card-number">6</div>
                    <span class="step-card-emoji">üì¶</span>
                    <div class="step-card-title">Batching</div>
                </div>
                <div class="step-card" onclick="goToStep(7)" data-step-card="7">
                    <div class="step-card-number">7</div>
                    <span class="step-card-emoji">üîÑ</span>
                    <div class="step-card-title">DataLoader</div>
                </div>
            </div>
        </div>

        <div class="main-content">
            <div class="visualization-area">
                <div id="visualization" class="viz-container"></div>
            </div>

            <div class="explanation-area">
                <div id="explanation">
                    <!-- Dynamic content will be inserted here -->
                </div>
            </div>
        </div>

        <div class="controls">
            <div class="control-section">
                <label class="control-label">Step Navigation</label>
                <div class="step-navigation">
                    <button class="nav-btn" onclick="previousStep()">‚óÑ Previous</button>
                    <button class="step-btn" data-step="1" onclick="goToStep(1)">1</button>
                    <button class="step-btn" data-step="2" onclick="goToStep(2)">2</button>
                    <button class="step-btn" data-step="3" onclick="goToStep(3)">3</button>
                    <button class="step-btn" data-step="4" onclick="goToStep(4)">4</button>
                    <button class="step-btn" data-step="5" onclick="goToStep(5)">5</button>
                    <button class="step-btn" data-step="6" onclick="goToStep(6)">6</button>
                    <button class="step-btn" data-step="7" onclick="goToStep(7)">7</button>
                    <button class="nav-btn" onclick="nextStep()">Next ‚ñ∫</button>
                    <button class="nav-btn play-pause-btn" onclick="togglePlay()" id="playPauseBtn">‚ñ∂ Play</button>
                </div>
            </div>

            <div class="control-section">
                <label class="control-label">Batch Size: <span class="slider-value" id="batchSizeValue">8</span></label>
                <div class="slider-container">
                    <input type="range" min="0" max="3" value="2" class="slider" id="batchSizeSlider" oninput="updateBatchSize(this.value)">
                    <span style="color: #64748b; font-size: 0.9rem;">2 ‚Üí 16</span>
                </div>
            </div>

            <div class="control-section">
                <label class="control-label">drop_last: <span id="dropLastValue" style="color: #8b5cf6; font-weight: 600;">True</span></label>
                <label class="toggle-switch">
                    <input type="checkbox" id="dropLastToggle" checked onchange="updateDropLast(this.checked)">
                    <span class="toggle-slider"></span>
                </label>
            </div>
        </div>
    </div>

    <!-- Code Modal -->
    <div id="codeModal" class="modal">
        <div class="modal-content">
            <div class="modal-header">
                <h2>Complete Code: Dataset & DataLoader</h2>
                <span class="close" onclick="closeModal()">&times;</span>
            </div>
            <div class="modal-body">
                <div class="code-container" id="codeContainer">
                    <!-- Code will be inserted here -->
                </div>
            </div>
        </div>
    </div>

    <script>
        // State management
        let currentStep = 1;
        let isPlaying = false;
        let playInterval = null;
        let currentBatchSize = 8;
        let dropLast = true;
        const maxLength = 120;

        const batchSizes = [2, 4, 8, 16];

        // Step data
        const steps = [
            {
                id: 1,
                emoji: "üì•",
                title: "Raw SMS Text",
                codeLines: [12],
                color: "#f97316",
                explanation: `
                    <h3>What's happening:</h3>
                    <ul>
                        <li>Data starts as human-readable text</li>
                        <li>Stored in a CSV file with columns: "Text" and "Label"</li>
                        <li>Labels are binary: 0 = not spam, 1 = spam</li>
                    </ul>
                    <h3>Why this matters:</h3>
                    <p>Neural networks cannot process text directly‚Äîthey only understand numbers. We need to convert these strings into numerical representations.</p>
                    <div class="code-example">self.data = pd.read_csv(csv_file)<br># Reads CSV containing columns: "Text", "Label"<br># Example row: "You are a winner...", 1</div>
                `
            },
            {
                id: 2,
                emoji: "üî¢",
                title: "Tokenization Process",
                codeLines: [15, 16],
                color: "#f97316",
                explanation: `
                    <h3>What's happening:</h3>
                    <ul>
                        <li>The tokenizer.encode() method breaks text into subword pieces</li>
                        <li>Each piece maps to a unique integer ID from the vocabulary (50,257 tokens for GPT-2)</li>
                        <li>Example: "winner" might become token ID 8464</li>
                    </ul>
                    <h3>Why this matters:</h3>
                    <p>Computers can't process words‚Äîthey need numbers. Subword tokenization handles unknown words gracefully. Each token ID represents a meaningful unit (word, subword, or character).</p>
                    <div class="code-example">self.encoded_texts = [tokenizer.encode(text) <br>                      for text in self.data["Text"]]<br># "You are a winner" ‚Üí [1639, 389, 257, 8464]</div>
                `
            },
            {
                id: 3,
                emoji: "üìè",
                title: "Finding Maximum Length",
                codeLines: [19, 20, 56, 57, 58, 59, 60, 61],
                color: "#8b5cf6",
                explanation: `
                    <h3>What's happening:</h3>
                    <ul>
                        <li>Iterate through all tokenized sequences</li>
                        <li>Find the longest sequence (becomes max_length)</li>
                        <li>In this example: max_length = 120 tokens</li>
                    </ul>
                    <h3>Why this matters:</h3>
                    <p>Batching requires all sequences to have the same shape. Too short ‚Üí we'll pad with special tokens. Too long ‚Üí we'll truncate to max_length.</p>
                    <h3>Memory Consideration:</h3>
                    <p>Longer sequences = more memory. Setting max_length balances information retention and computational efficiency.</p>
                    <div class="code-example">if max_length is None:<br>    self.max_length = self._longest_encoded_length()</div>
                `
            },
            {
                id: 4,
                emoji: "‚öñÔ∏è",
                title: "Padding to Fixed Length",
                codeLines: [27, 28, 29],
                color: "#8b5cf6",
                explanation: `
                    <h3>What's happening:</h3>
                    <ul>
                        <li>Short sequences get padding tokens appended</li>
                        <li>Padding token ID: 50256 (GPT-2's &lt;|endoftext|&gt; token)</li>
                        <li>Formula: padded = original + [50256] √ó (120 - len(original))</li>
                    </ul>
                    <h3>Why this matters:</h3>
                    <p>Creates uniform tensor shapes for batching. The model learns to ignore padding tokens during training. Without padding, we can't stack sequences into a batch tensor.</p>
                    <h3>Visual Example:</h3>
                    <div class="code-example">Original:  [1639, 389, 257, 8464]  (4 tokens)<br>Padded:    [1639, 389, 257, 8464, 50256, 50256, ..., 50256]  (120 tokens)<br>                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 116 padding tokens ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</div>
                `
            },
            {
                id: 5,
                emoji: "üéØ",
                title: "Dataset __getitem__ Method",
                codeLines: [32, 34, 37, 38, 39, 40, 41],
                color: "#14b8a6",
                explanation: `
                    <h3>What's happening:</h3>
                    <ul>
                        <li>Input: integer index (e.g., 0, 1, 2, ...)</li>
                        <li>Process: Get encoded text and label at that index</li>
                        <li>Output: tuple of (input_tensor, label_tensor)</li>
                    </ul>
                    <h3>Why this matters:</h3>
                    <p>PyTorch's DataLoader calls __getitem__ repeatedly. This method determines what one "sample" looks like. Tensors are returned (not Python lists) for GPU compatibility.</p>
                    <h3>Data Types:</h3>
                    <ul>
                        <li>input_tensor: torch.long (int64) for token IDs</li>
                        <li>label_tensor: torch.long (int64) for class labels</li>
                    </ul>
                    <div class="code-example">dataset[0] ‚Üí (torch.tensor([1639, 389, ..., 50256]), torch.tensor(1))<br>              ‚Üë                                         ‚Üë<br>              120 token IDs                             spam label</div>
                `
            },
            {
                id: 6,
                emoji: "üì¶",
                title: "Batch Creation",
                codeLines: [66, 67, 68, 69, 70, 71, 72, 73],
                color: "#14b8a6",
                explanation: `
                    <h3>What's happening:</h3>
                    <ul>
                        <li>DataLoader repeatedly calls dataset[index]</li>
                        <li>Collects batch_size=${currentBatchSize} samples</li>
                        <li>Stacks tensors along a new batch dimension</li>
                    </ul>
                    <h3>Tensor Shape Evolution:</h3>
                    <div class="code-example">Sample 0: [120] token IDs<br>Sample 1: [120] token IDs<br>...<br>Sample ${currentBatchSize-1}: [120] token IDs<br>      ‚Üì Stack<br>Batch: [${currentBatchSize}, 120] tensor</div>
                    <h3>Why this matters:</h3>
                    <p>Batch processing is much faster than processing one sample at a time. GPUs are optimized for parallel matrix operations. Batch size is a hyperparameter (typical values: 8, 16, 32, 64).</p>
                    <h3>Memory Layout:</h3>
                    <p>A batch of ${currentBatchSize} sequences with max_length=120:<br>
                    ‚Ä¢ Input batch shape: (${currentBatchSize}, 120)<br>
                    ‚Ä¢ Label batch shape: (${currentBatchSize},)<br>
                    ‚Ä¢ ${currentBatchSize} samples √ó 120 tokens = ${currentBatchSize * 120} integers in input batch</p>
                `
            },
            {
                id: 7,
                emoji: "üîÑ",
                title: "DataLoader Iteration",
                codeLines: [76, 77],
                color: "#8b5cf6",
                explanation: `
                    <h3>What's happening:</h3>
                    <ul>
                        <li><code>for input_batch, label_batch in train_loader:</code> triggers iteration</li>
                        <li>Each iteration yields one batch</li>
                        <li>Shuffle randomizes sample order (important for training)</li>
                        <li>drop_last controls incomplete final batches</li>
                    </ul>
                    <h3>Key Parameters:</h3>
                    <p><strong>shuffle=True</strong> (training):</p>
                    <ul>
                        <li>Randomizes data order each epoch</li>
                        <li>Prevents model from learning spurious patterns</li>
                        <li>Different batch composition every epoch</li>
                    </ul>
                    <p><strong>drop_last=${dropLast}</strong> ${dropLast ? '(training)' : '(validation/test)'}:</p>
                    <ul>
                        <li>${dropLast ? 'Discards incomplete final batch' : 'Keeps all samples for complete evaluation'}</li>
                        <li>${dropLast ? 'Ensures consistent batch sizes for Batch Normalization' : 'Last batch may be smaller'}</li>
                        <li>Example: 45 samples √∑ 8 batch_size = ${dropLast ? '5 batches (40 samples used, 5 dropped)' : '6 batches (5 in last)'}</li>
                    </ul>
                    <h3>Lazy Loading:</h3>
                    <p>Batches are created only when iterated‚Äînot when DataLoader is instantiated. This saves memory for large datasets.</p>
                `
            }
        ];

        // SMS examples
        const smsExamples = [
            { text: "Congratulations! You've won $5000. Call now: 555-CASH", label: 1 },
            { text: "Hey, running 10 mins late. Start without me.", label: 0 },
            { text: "URGENT: Your bank account needs verification. Click: bit.ly/fake", label: 1 },
            { text: "Can you pick up milk on your way home?", label: 0 }
        ];

        // Complete code
        const completeCode = `import torch
from torch.utils.data import Dataset
import pandas as pd

class SpamDataset(Dataset):
    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):
        """
        Initializes the dataset by reading text data, tokenizing it, 
        padding or truncating sequences, and assigning labels.
        """
        # Step 1: Read CSV
        self.data = pd.read_csv(csv_file)
        
        # Step 2: Tokenize
        self.encoded_texts = [tokenizer.encode(text) 
                              for text in self.data["Text"]]
        
        # Step 3: Find max length
        if max_length is None:
            self.max_length = self._longest_encoded_length()
        else:
            self.max_length = max_length
            self.encoded_texts = [encoded_text[:self.max_length] 
                                  for encoded_text in self.encoded_texts]
        
        # Step 4: Pad sequences
        self.encoded_texts = [
            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text)) 
            for encoded_text in self.encoded_texts
        ]
    
    def __getitem__(self, index):
        # Step 5: Retrieve single sample
        encoded = self.encoded_texts[index]
        
        # Retrieve the corresponding label
        label = self.data.iloc[index]["Label"]
        
        # Convert both to PyTorch tensors
        return (
            torch.tensor(encoded, dtype=torch.long),
            torch.tensor(label, dtype=torch.long)
        )
    
    def __len__(self):
        return len(self.data)
    
    def _longest_encoded_length(self):
        # Step 3: Helper method
        max_length = 0
        for encoded_text in self.encoded_texts:
            encoded_length = len(encoded_text)
            if encoded_length > max_length:
                max_length = encoded_length
        return max_length

# Steps 6-7: DataLoader creation
from torch.utils.data import DataLoader

batch_size = 8
train_loader = DataLoader(
    dataset=train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=0,
    drop_last=True
)

# Step 7: Iteration
for input_batch, target_batch in train_loader:
    pass`;

        // Render functions for each step
        function renderStep1() {
            return `
                ${smsExamples.map((sms, idx) => `
                    <div class="sms-box" style="animation-delay: ${idx * 0.1}s">
                        <strong>${sms.label === 1 ? 'üö® SPAM:' : '‚úÖ NOT SPAM:'}</strong><br>
                        "${sms.text}"
                    </div>
                `).join('')}
            `;
        }

        function renderStep2() {
            return `
                <div style="text-align: center;">
                    <div class="sms-box" style="max-width: 500px;">
                        "You are a winner! Claim your prize"
                    </div>
                    <div class="arrow">‚Üì</div>
                    <div style="font-size: 1.2rem; color: #8b5cf6; font-weight: 600; margin: 8px 0;">
                        Tokenization
                    </div>
                    <div class="arrow">‚Üì</div>
                    <div class="token-flow">
                        <div class="token">1639</div>
                        <div class="token">389</div>
                        <div class="token">257</div>
                        <div class="token">8464</div>
                        <div class="token">0</div>
                        <div class="token">...</div>
                    </div>
                    <div style="margin-top: 15px; color: #64748b; font-style: italic; font-size: 0.85rem;">
                        Each word/subword ‚Üí unique token ID
                    </div>
                </div>
            `;
        }

        function renderStep3() {
            const sequences = [
                { length: 45, maxLength: 120 },
                { length: 89, maxLength: 120 },
                { length: 120, maxLength: 120 },
                { length: 67, maxLength: 120 }
            ];
            
            return `
                <div style="width: 100%; max-width: 600px;">
                    <div style="text-align: center; margin-bottom: 15px;">
                        <div style="font-size: 1.1rem; color: #8b5cf6; font-weight: 600;">
                            Finding max_length = 120 tokens
                        </div>
                    </div>
                    ${sequences.map((seq, idx) => `
                        <div class="sequence-bar" style="animation-delay: ${idx * 0.15}s">
                            <div class="sequence-label">Seq ${idx + 1}:</div>
                            <div class="bar-container">
                                <div class="bar-filled" style="width: ${(seq.length / seq.maxLength) * 100}%"></div>
                            </div>
                            <div class="length-label">${seq.length} tokens</div>
                        </div>
                    `).join('')}
                    <div style="text-align: center; margin-top: 20px; padding: 12px; background: #fef3c7; border-radius: 8px;">
                        <strong style="color: #92400e; font-size: 0.9rem;">Maximum Length: 120 tokens</strong>
                    </div>
                </div>
            `;
        }

        function renderStep4() {
            return `
                <div class="padding-demo">
                    <div class="padding-sequence">
                        <div style="font-weight: 600; color: #64748b; margin-bottom: 8px; font-size: 0.85rem;">
                            Original (8 tokens):
                        </div>
                        <div class="sequence-tokens">
                            ${Array(8).fill(0).map((_, i) => `
                                <div class="token-small token-actual">${1639 + i}</div>
                            `).join('')}
                        </div>
                    </div>
                    
                    <div style="text-align: center; font-size: 1.5rem; color: #8b5cf6;">‚¨á</div>
                    
                    <div class="padding-sequence">
                        <div style="font-weight: 600; color: #64748b; margin-bottom: 8px; font-size: 0.85rem;">
                            After Padding (120 tokens):
                        </div>
                        <div class="sequence-tokens">
                            ${Array(8).fill(0).map((_, i) => `
                                <div class="token-small token-actual">${1639 + i}</div>
                            `).join('')}
                            ${Array(20).fill(0).map(() => `
                                <div class="token-small token-pad">50256</div>
                            `).join('')}
                            <div style="color: #64748b; padding: 5px; font-size: 0.8rem;">...</div>
                            ${Array(4).fill(0).map(() => `
                                <div class="token-small token-pad">50256</div>
                            `).join('')}
                        </div>
                        <div style="margin-top: 8px; color: #64748b; font-size: 0.8rem;">
                            + 112 padding tokens
                        </div>
                    </div>
                </div>
            `;
        }

        function renderStep5() {
            return `
                <div class="dataset-diagram">
                    <div style="font-size: 1.1rem; color: #1e293b; font-weight: 600; margin-bottom: 15px;">
                        Dataset.__getitem__(index)
                    </div>
                    
                    <div style="background: #fef3c7; padding: 15px 25px; border-radius: 12px; border: 2px solid #fbbf24;">
                        <code style="font-size: 0.95rem; color: #1e293b;">index = 0</code>
                    </div>
                    
                    <div class="arrow">‚Üì</div>
                    
                    <div class="dataset-box">
                        Dataset Class
                        <div style="font-size: 0.8rem; margin-top: 8px; opacity: 0.9;">
                            Retrieves sample at index
                        </div>
                    </div>
                    
                    <div class="arrow">‚Üì</div>
                    
                    <div style="display: flex; gap: 15px;">
                        <div class="shape-display">
                            torch.tensor([1639, 389, ...])<br>
                            <span style="color: #64748b; font-size: 0.8rem;">120 token IDs</span>
                        </div>
                        <div class="shape-display">
                            torch.tensor(1)<br>
                            <span style="color: #64748b; font-size: 0.8rem;">label (spam)</span>
                        </div>
                    </div>
                </div>
            `;
        }

        function renderStep6() {
            return `
                <div class="batch-stack" style="width: 100%;">
                    <div style="font-size: 1.1rem; color: #1e293b; font-weight: 600; margin-bottom: 15px;">
                        Stacking ${currentBatchSize} Samples into Batch
                    </div>
                    
                    ${Array(Math.min(currentBatchSize, 4)).fill(0).map((_, i) => `
                        <div class="batch-item">
                            Sample ${i}: [120 tokens]
                        </div>
                    `).join('')}
                    
                    ${currentBatchSize > 4 ? `
                        <div style="color: #64748b; font-size: 1rem; margin: 8px 0;">...</div>
                        <div class="batch-item" style="animation-delay: 0.5s;">
                            Sample ${currentBatchSize - 1}: [120 tokens]
                        </div>
                    ` : ''}
                    
                    <div style="font-size: 1.5rem; color: #8b5cf6; margin: 15px 0;">‚¨á</div>
                    
                    <div class="shape-display" style="font-size: 1rem;">
                        Batch Shape: torch.Size([${currentBatchSize}, 120])
                    </div>
                    
                    <div style="margin-top: 15px; text-align: center; color: #64748b; font-size: 0.85rem;">
                        <strong>${currentBatchSize}</strong> samples √ó <strong>120</strong> tokens = <strong>${currentBatchSize * 120}</strong> integers
                    </div>
                </div>
            `;
        }

        function renderStep7() {
            const totalSamples = 45;
            const numBatches = dropLast ? Math.floor(totalSamples / currentBatchSize) : Math.ceil(totalSamples / currentBatchSize);
            const samplesUsed = dropLast ? numBatches * currentBatchSize : totalSamples;
            const lastBatchSize = dropLast ? currentBatchSize : (totalSamples % currentBatchSize || currentBatchSize);
            
            return `
                <div class="dataloader-loop">
                    <div style="font-size: 1.1rem; color: #1e293b; font-weight: 600; text-align: center;">
                        DataLoader Iteration
                    </div>
                    
                    <div class="loop-iteration">
                        <div class="iteration-header">Configuration:</div>
                        <div style="color: #64748b; margin-top: 8px; line-height: 1.6; font-size: 0.85rem;">
                            ‚Ä¢ batch_size = ${currentBatchSize}<br>
                            ‚Ä¢ shuffle = True<br>
                            ‚Ä¢ drop_last = ${dropLast}<br>
                            ‚Ä¢ Total samples = ${totalSamples}
                        </div>
                    </div>
                    
                    <div class="loop-iteration">
                        <div class="iteration-header">Result:</div>
                        <div style="color: #64748b; margin-top: 8px; line-height: 1.6; font-size: 0.85rem;">
                            ‚Ä¢ Number of batches: <strong>${numBatches}</strong><br>
                            ‚Ä¢ Samples used: <strong>${samplesUsed}</strong> / ${totalSamples}<br>
                            ‚Ä¢ Last batch size: <strong>${lastBatchSize}</strong><br>
                            ${dropLast && (totalSamples % currentBatchSize !== 0) ? `‚Ä¢ Samples dropped: <strong>${totalSamples - samplesUsed}</strong>` : ''}
                        </div>
                    </div>
                    
                    <div class="shape-display" style="margin-top: 15px; font-size: 0.9rem;">
                        for input_batch, label_batch in train_loader:<br>
                        <span style="color: #64748b; font-size: 0.85rem; margin-left: 20px;">
                            # input_batch.shape: [${currentBatchSize}, 120]<br>
                            # label_batch.shape: [${currentBatchSize}]
                        </span>
                    </div>
                </div>
            `;
        }

        // Main render function
        function renderVisualization(stepId) {
            const vizContainer = document.getElementById('visualization');
            vizContainer.classList.add('fade-exit');
            
            setTimeout(() => {
                let content = '';
                switch(stepId) {
                    case 1: content = renderStep1(); break;
                    case 2: content = renderStep2(); break;
                    case 3: content = renderStep3(); break;
                    case 4: content = renderStep4(); break;
                    case 5: content = renderStep5(); break;
                    case 6: content = renderStep6(); break;
                    case 7: content = renderStep7(); break;
                }
                
                vizContainer.innerHTML = content;
                vizContainer.classList.remove('fade-exit');
                vizContainer.classList.add('fade-enter');
                
                setTimeout(() => {
                    vizContainer.classList.remove('fade-enter');
                }, 600);
            }, 300);
        }

        function renderExplanation(stepId) {
            const step = steps[stepId - 1];
            const explanationContainer = document.getElementById('explanation');
            
            explanationContainer.innerHTML = `
                <div class="step-title">
                    <span class="step-emoji">${step.emoji}</span>
                    <span>STEP ${step.id}: ${step.title}</span>
                </div>
                <div class="explanation-text">
                    ${step.explanation}
                </div>
                <button class="view-code-btn" onclick="openCodeModal(${stepId})">
                    üìÑ View in Context
                </button>
                ${stepId === 7 ? renderKeyTakeaways() : ''}
            `;
        }

        function renderKeyTakeaways() {
            return `
                <div class="key-takeaways">
                    <h3>üéì Key Takeaways: Complete Pipeline Understanding</h3>
                    <ul class="takeaway-list">
                        <li><strong>Text ‚Üí Numbers</strong>: Tokenization converts strings to numerical IDs</li>
                        <li><strong>Standardization</strong>: Padding ensures uniform tensor shapes</li>
                        <li><strong>Dataset Organization</strong>: __getitem__ defines sample retrieval logic</li>
                        <li><strong>Batching Efficiency</strong>: Groups samples for parallel GPU processing</li>
                        <li><strong>Iteration Pattern</strong>: DataLoader lazily creates batches on-demand</li>
                    </ul>
                    <div style="margin-top: 15px; padding: 12px; background: white; border-radius: 8px; color: #064e3b; font-size: 0.85rem; line-height: 1.5;">
                        <strong>Real-World Impact:</strong> This exact pattern is used in every PyTorch NLP project‚Äîfrom BERT fine-tuning to GPT training. Understanding this flow is foundational for deep learning practitioners.
                    </div>
                </div>
            `;
        }

        // Navigation functions
        function goToStep(stepId) {
            currentStep = stepId;
            updateStepButtons();
            renderVisualization(stepId);
            renderExplanation(stepId);
        }

        function nextStep() {
            if (currentStep < 7) {
                goToStep(currentStep + 1);
            }
        }

        function previousStep() {
            if (currentStep > 1) {
                goToStep(currentStep - 1);
            }
        }

        function updateStepButtons() {
            // Update bottom navigation buttons
            document.querySelectorAll('.step-btn').forEach(btn => {
                const step = parseInt(btn.getAttribute('data-step'));
                if (step === currentStep) {
                    btn.classList.add('active');
                } else {
                    btn.classList.remove('active');
                }
            });
            
            // Update top overview step cards
            document.querySelectorAll('.step-card').forEach(card => {
                const step = parseInt(card.getAttribute('data-step-card'));
                if (step === currentStep) {
                    card.classList.add('active-card');
                } else {
                    card.classList.remove('active-card');
                }
            });
        }

        function togglePlay() {
            isPlaying = !isPlaying;
            const btn = document.getElementById('playPauseBtn');
            
            if (isPlaying) {
                btn.textContent = '‚ùö‚ùö Pause';
                btn.style.background = '#ef4444';
                playInterval = setInterval(() => {
                    if (currentStep < 7) {
                        nextStep();
                    } else {
                        togglePlay();
                    }
                }, 3000);
            } else {
                btn.textContent = '‚ñ∂ Play';
                btn.style.background = '#14b8a6';
                clearInterval(playInterval);
            }
        }

        function updateBatchSize(sliderValue) {
            currentBatchSize = batchSizes[sliderValue];
            document.getElementById('batchSizeValue').textContent = currentBatchSize;
            
            if (currentStep === 6 || currentStep === 7) {
                renderVisualization(currentStep);
                renderExplanation(currentStep);
            }
        }

        function updateDropLast(checked) {
            dropLast = checked;
            document.getElementById('dropLastValue').textContent = checked ? 'True' : 'False';
            
            if (currentStep === 7) {
                renderVisualization(currentStep);
                renderExplanation(currentStep);
            }
        }

        // Code modal functions
        function openCodeModal(stepId) {
            const modal = document.getElementById('codeModal');
            const codeContainer = document.getElementById('codeContainer');
            const step = steps[stepId - 1];
            
            // Render code with line numbers
            const lines = completeCode.split('\n');
            let codeHTML = '';
            
            lines.forEach((line, idx) => {
                const lineNum = idx + 1;
                const isHighlighted = step.codeLines.includes(lineNum);
                const highlightClass = isHighlighted ? 'highlight' : '';
                
                codeHTML += `
                    <div class="code-line ${highlightClass}" data-line="${lineNum}" id="line-${lineNum}">
                        <div class="line-number">${lineNum}</div>
                        <div class="line-content">${escapeHtml(line)}</div>
                    </div>
                `;
            });
            
            codeContainer.innerHTML = codeHTML;
            modal.style.display = 'block';
            
            // Scroll to first highlighted line
            setTimeout(() => {
                if (step.codeLines.length > 0) {
                    const firstLine = document.getElementById(`line-${step.codeLines[0]}`);
                    if (firstLine) {
                        firstLine.scrollIntoView({ behavior: 'smooth', block: 'center' });
                    }
                }
            }, 100);
        }

        function closeModal() {
            document.getElementById('codeModal').style.display = 'none';
        }

        function escapeHtml(text) {
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }

        // Close modal when clicking outside
        window.onclick = function(event) {
            const modal = document.getElementById('codeModal');
            if (event.target === modal) {
                closeModal();
            }
        }

        // Initialize
        document.addEventListener('DOMContentLoaded', () => {
            goToStep(1);
        });
    </script>
</body>
</html>